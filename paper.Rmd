---
title: " Casting multiple shadows: interactive data visualisation with tours and embeddings"
author:
  - name: Stuart Lee
    affiliation: Monash University
  - name: Di Cook
    affiliation: Monash University
date: "`r Sys.Date()`"
output: distill::distill_article
bibliography: liminal.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

<!--
- review of embedding algorithms, interactive graphics, tours
- technology
- design
- examples/benchmarks
  - Kevin moon
  - pdfsense
  - single cell
  - quick draw
-->

# Introduction

High dimensional data is increasingly prevalent in the natural sciences and
beyond but presents a challenge to the analyst in terms of both data cleaning /
preprocessing and visualisation. Methods to embed data from a high-dimesional
space into a low-dimensional one now form a core step of the data analysis
workflow where they are used to ascertain hidden structure and denoise data for
downstream analysis (thereby nullifying the 'curse of dimensionality').

Choosing an appropriate embedding presents a challenge to the analyst. How does
an analyst know whether the embedding has captured the underlying topology and
geometry of the high dimensional space? The answer depends on the analyst's
workflow.  @Brehmer2014-hk characterised two main workflow steps that an
analyst performs when using embedding techniques: dimension reduction and
cluster orientation. The first relates to dimension reduction achieved by using
an embedding method, here an analyst wants to characterise and map meaning onto
the embedded form, for example identifying batch effects from a high throughput
sequencing experiment, or identifying a gradient or trajectory along the
embedded form. The second relates to using embeddings as part of a clustering
workflow. Here analysts are interested in identifying and naming clusters and
verifying them by either applying known labels or colouring by variables that
are a-priori known to distinguish clusters. Both of these workflow steps rely
on the embedding being 'faithful' or the original high dimensional dataset, and
become much more difficult when there is no underlying ground truth.

Embedding methods can be classified into two broad groups: linear
and non-linear methods. Linear methods perform a linear transformation
of the data; one example is principal components analysis (PCA)
which performs an eigendecomposition of the estimated sample covariance matrix.
The eigenvalues are sorted in decreasing order and represent the variance
explained by each component (eigenvector). A common approach to deciding on
the number of principal components to retain is to plot the proportion of variance explained by each component and choose a cut-off. 

Non-linear methods
generally perform preprocessing on the high-dimensional data such as generating
a neighborhood graph and perform transformations on the preprocessed form. We
restrict our attention to three methods that are commonly used in
high-throughput biology: t-distributed stochastic neighbor embedding (t-SNE), 
uniform manifold alignment and projection (UMAP), and potential of heat-diffusion for affinity-based transition embedding (PHATE). The t-SNE algorithm estimates the similarity of (Euclidean) distances of points in a high dimensional space using a Gaussian distribution and then  
estimates a configuration in the low dimensional embedding space by modelling
similarities using a t-distribution with 1 degree of freedom. The resulting
configuration is the one that minimizes the Kulback-Leibler divergence
between the two distributions. A recent theoretical contribution by @Linderman2019-dq proved that t-SNE can recover spherical and well separated 
cluster shapes, and proposed new approaches for tuning the optimisation parameters. It is a known problem that t-SNE can have trouble recovering
global structure and that configurations can be highly dependent on
how the algorithm is initialised and parameterised 
[@Wattenberg2016-ji; @Kobak2019-lm]. UMAP is a method that is
related to LargeVis, and like t-SNE acts on the k-nearest neighbor graph. Its
main differences are that it uses a different cost function (cross entropy) which is optimized using stochastic gradient descent and defines a different
kernel for similiarites in the low dimensional space. Due to it's computational
speed it's possible to generate UMAP embeddings in more than three dimensions. Finally, PHATE, inspired by diffusion maps, is based on estimating an affinity matrix via a distance matrix and k-nearest neighbors graph. The algorithm
denoises estimated distances in high dimensional space via transforming the
affinity matrix into a Markov transition probaility matrix and diffusing this 
matrix over a fixed number of time steps. Then the diffused probabilities
are transformed once more to construct a distance matrix, and multidimensional
scaling is performed to construct a 2d embedding for visualization.  

As part of a visualization workflow, it's important to consider the perception 
and interpretation of embedding methods as well. @Sedlmair2013-pn showed that 2D
scatter plots were mostly suffucient for detecting class seperation, however
they noted that often multiple embeddings were required. For the task of cluster
identification, @Lewis2012-ai showed experimentally that novice
users of non-linear embedding techniques were more likely to consider clusters
of points on a 2d scatter plot to be the result of a spurious embedding
compared to advanced users who were aware of the inner workings of the embedding
algorithm. 

## Tours

## Interactive graphics


There is no one-size fits all: finding an appropriate embedding for a given
dataset is a difficult and somewhat poorly defined problem. For non-linear methods, there are a lot of parameters to explore that can have an effeft on the resulting visualisation and interpretation. We propose a more pragmatic workflow inspired by (wickham blindfold paper) by incorporating
interactive graphics and tours with embeddings that allows users to see a
global overview of their high dimensional data and assists them with cluster
orientation tasks. This workflow is incorporated into an R package
called `liminal` (Available: https://github.com/sa-lee/liminal).


https://jlmelville.github.io/smallvis/init.html#other_initialization_options




# Methods

my contribution + design of interface

## Tours as a streaming data problem


## Multiple views, multiple contexts



## Interactions and visual analytics for embeddings


# Results

## Case Study 1: Fake trees

## Case Study 2: well-clustered smallish single cell

## Case Study 3: largeish single cell data

## Case Study 4: non-spherical clusters, pdfsense


# Discussion

# References
