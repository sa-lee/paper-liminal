---
title: " Casting multiple shadows: high-dimensional interactive data visualisation with tours and embeddings"
author:
  - name: Stuart Lee
  - name: Di Cook
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    toc: false
    number_sections: no
bibliography: liminal.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


There has been a rapid uptake in the use of non-linear dimensionality reduction
(NLDR) methods such as t-distributed stochastic neighbour embedding (t-SNE) in
the natural sciences as part of cluster orientation and dimension reduction
workflows. The appropriate use of these methods is made difficult by their
complex parameterisations and the multitude of decisions required to balance
the preservation of local and global structure in the resulting visualisation.
We present a visual analytics framework for the pragmatic usage of NLDR methods
by combining them with a technique called the tour. A tour is a sequence of
interpolated linear projections of multivariate data onto a lower dimensional
space. The sequence is displayed as a dynamic visualisation, allowing a user to
see the shadows the high-dimensional data casts in a lower dimensional view. By
linking the tour to a view obtained from an NLDR method, we can preserve global
structure and through user interactions like spatial linked brushing observe
where the NLDR view may be misleading. We display several case studies from
single cell transciptomics and image processing, that shows our approach is 
useful for cluster orientation tasks.  The implementation of our framework 
is available as an R package called
`liminal` available at https://github.com/sa-lee/liminal.


## Introduction

High dimensional data is increasingly prevalent in the natural sciences and
beyond but presents a challenge to the analyst in terms of both data cleaning /
pre-processing and visualisation. Methods to embed data from a high-dimensional
space into a low-dimensional one now form a core step of the data analysis
workflow where they are used to ascertain hidden structure and de-noise data
for downstream analysis (thereby nullifying the 'curse of dimensionality').

Choosing an appropriate embedding presents a challenge to the analyst. How does
an analyst know whether the embedding has captured the underlying topology and
geometry of the high dimensional space? The answer depends on the analyst's
workflow.  @Brehmer2014-hk characterised two main workflow steps that an
analyst performs when using embedding techniques: dimension reduction and
cluster orientation. The first relates to dimension reduction achieved by using
an embedding method, here an analyst wants to characterise and map meaning onto
the embedded form, for example identifying batch effects from a high throughput
sequencing experiment, or identifying a gradient or trajectory along the
embedded form @Nguyen2019-yh. The second relates to using embeddings as part of
a clustering workflow. Here analysts are interested in identifying and naming
clusters and verifying them by either applying known labels or colouring by
variables that are a-priori known to distinguish clusters. Both of these
workflow steps rely on the embedding being 'faithful' or the original high
dimensional dataset, and become much more difficult when there is no underlying
ground truth.

As part of a visualization workflow, it's important to consider the perception
and interpretation of embedding methods as well. @Sedlmair2013-pn showed that
2D scatter plots were mostly sufficient for detecting class separation, however
they noted that often multiple embeddings were required. For the task of
cluster identification, @Lewis2012-ai showed experimentally that novice users
of non-linear embedding techniques were more likely to consider clusters of
points on a 2d scatter plot to be the result of a spurious embedding compared
to advanced users who were aware of the inner workings of the embedding
algorithm.

A complementary approach for visualizing structure in high dimensional data is
the tour. A tour is a sequence of projections of a high dimensional dataset
onto a low-dimensional orthonormal basis matrix, that is represented as a
dynamic visualization. The sequence of generated bases are interpolated to form
the tour path, allowing a user to explore the subspace of projections. A grand
tour corresponds to choosing new bases at random, and can give an overview of
the structure in the data. Instead of picking projections at random, a guided
tour can be used to generate a sequence 'interesting' projections as quantified
by an index function. Given the dynamic nature of the tour, user interaction is
important for controlling and exploring the visualisation: the tour has been
used previously by @Wickham2015-cx as tool for exploring statistical model fits
and by @Buja1996-fk for exploring factorial experimental designs.

While there has been much work on the algorithmic details of the aforementioned
embedding methods, there has been relatively few tools designed to assist users
to interact with these techniques and assist them in making comparisons between
embeddings and performing the aforementioned cluster orientation tasks. Several
interactive interfaces have been proposed for evaluating or using embedding
techniques: the Sleepwalk interface provides a click and highlight
visualisation for colouring points in an embedding according to their distance
in the original high-dimensional space [@Ovchinnikova2019-gf]. The work by
@Pezzotti2017-cz provides a user guided and modified form of the t-SNE
algorithm, that allows users to modified optimisation parameters in real-time.
Similarly, the embedding projector is a web interface to running UMAP, t-SNE or
PCA live in the browser and provides interactions to color points, and
highlights nearest neighbors [@Smilkov2016-hp].

There is no one-size fits all: finding an appropriate embedding for a given
dataset is a difficult and somewhat poorly defined problem. For non-linear
methods, there are a lot of parameters to explore that can have an effect on
the resulting visualisation and interpretation. Interfaces for evaluating
embeddings require interaction but should also be able to be incorporated into
an analysts workflow. We propose a more pragmatic workflow inspired by
incorporating interactive graphics and tours with embeddings that allows users
to see a global overview of their high dimensional data and assists them with
cluster orientation tasks. This workflow is incorporated into an R package
called `liminal` (Available: https://github.com/sa-lee/liminal).

## Dimensionality Reduction Overview

In the following section, we provide a brief overview of the goals
of DR methods with respect to the data analyst, in addition to their high level the 
mathematical details. To begin we suppose the data is in the form 
rectangular matrix of real numbers, $X = [\mathbf{x_1}, \dots, \mathbf{x_n}]$, 
where $n$ is the number of observations in $p$ dimensions. 
The purpose of any DR algorithm is to find low-dimensonal
representation of the data, $Y = [\mathbf{y_1}, \dots, \mathbf{y_n}]$, such that
$Y$ is an $n \times d$ matrix where $d \ll p$. The hope of the analyst is that
the DR procedure to produces $Y$ will remove noise in the original dataset while
retaining latent structure.

DR methods can be classified into two broad groups: linear and
non-linear methods. Linear methods perform a linear transformation of the data,
that is, $Y$ is a linear transformation of $X$;
one example is principal components analysis (PCA) which performs an
eigendecomposition of the estimated sample covariance matrix. The eigenvalues
are sorted in decreasing order and represent the variance explained by each
component (eigenvector). A common approach to deciding on the number of
principal components to retain is to plot the proportion of variance explained
by each component and choose a cut-off.

For non-linear methods $Y$ is generated via a pre-processed form of the input 
$X$ such as the $k$-nearest neighbours graph or via a kernel transformation. 
Here we restrict our attention to two recent methods that are commonly
used in the literature: t-distributed stochastic neighbor embedding (t-SNE) and
uniform manifold alignment and projection (UMAP). The t-SNE algorithm estimates
the pairwise similarity of (Euclidean) distances of points in a high
dimensional space using a Gaussian distribution and then estimates a
configuration in the low dimensional embedding space by modelling similarities
using a t-distribution with 1 degree of freedom. There are several subtleties
to the to use of the algorithm that are revealed by stepping through its
machinery.

To begin, t-SNE transforms pairwise distances between   $\mathbf{x_i}$ and 
$\mathbf{x_j}$  to similiarties using a Gaussian kernel:

$$
p_{i|j} = \frac{\exp(-\lVert \mathbf{x_i - x_j} \rVert ^ 2 / 2\sigma_i^2)}{\sum_{k \ne i}\exp(-\lVert \mathbf{x_j - x_k} \rVert ^ 2 / 2\sigma_i^2)}
$$ 

The conditional probabilities are then normalised and symmetrised to form a 
joint probability  distrubition via averaging:

$$
p_{ij} = \frac{p_{i|j} + p_{j|i}}{2n}
$$

The variance parameter of the 
Gaussian kernel is controlled by the analyst using a fixed value of perplexity
for all observations:

$$
\text{perplexity}_i = \exp(-\log(2) \sum_{i \ne j}p_{j|i}\log_2(p_{j|i}))
$$
As the perplexity increases, $\sigma^2_{i}$ increases, until its bounded
above by the number of observations , $n-1$, in the data, corresponding to  
$\sigma^2_{i} \rightarrow \infty$. This essentially turns $t-SNE$ into a 
nearest neighbours algorithm, $p_{i|j}$ will be close to zero for all 
observations that are not in the $\mathcal{O}(\text{perplexity}_i)$
neighbourhood graph of the $i$th observation [@Van_Der_Maaten2014-zn].

Next, the target low-dimensional space, $Y$, pairwise distances between 
$\mathbf{y_i}$ and $\mathbf{y_j}$ are modelled as a symmetric probability 
distrubtion using a Cauchy kernel:

$$
q_{ij} = \frac{w_{ij}}{Z} \\
\text{where } w_{ij} = \frac{1}{ 1 + \lVert \mathbf{y_i - y_j} \rVert ^ 2} 
\text{ and } Z = \sum_{k \ne l} w_{kl}.
$$ 


The resulting configuration $Y$ is
the one that minimizes the Kullback-Leibler divergence between 
the probability distrubtions formed via similiarties of observations in $X$, $\mathcal{P}$ 
and similiarities of observations in $Y$, $\mathcal{Q}$:

$$ \mathcal{L(\mathcal{P}, \mathcal{Q})} = \sum_{i \ne j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$$

Re-writing the loss function in terms of attractive (right) and repulsive (left) 
forces we obtain:

$$
\mathcal{L(\mathcal{P}, \mathcal{Q})} = -\sum_{i \ne j} p_{ij}\log w_{ij} + \log\sum_{i \ne j} w_{ij}
$$

So when the the loss function is minimised this corresponds to large
attractive forces, that is, the pairwise distances in $Y$ are small when
there are non-zero $p_{ij}$, i.e. $x_i$ and $x_j$ are close together. The
repulsive force should also be small when the loss function is minimised, 
that is, when pairwise distances in $Y$ are large irregardless of the magnitude
of the corresponding distances in $X$. 

Taken together, these details reveal the sheer number of decisions that an 
anlayst must make. How does one choose the perplexity? How should the parameters
that control the optimisation of the loss function (done with stochastic
gradient descent), like the number of iterations, or early exaggeration (
a multiplier of the attractive force at the beginning of the optimisation),
or the learning rate be selected?  It is a known
problem that t-SNE can have trouble recovering topology and that
configurations can be highly dependent on how the algorithm is initialised and
parameterized [@Wattenberg2016-ji; @Kobak2019-lm; @Melville2020]. If the goal is
cluster orientation a recent theoretical contribution by @Linderman2019-dq proved
that t-SNE can recover spherical and well separated cluster shapes, and
proposed new approaches for tuning the optimisation parameters. However,
the cluster sizes and their relative orientation from a $t-SNE$ view can be 
misleading perceptually, due to the algorithms emphasis on locality.

Another recent method UMAP has seen a large rise in popularity (at least in
single cell transcriptomics) [@McInnes2018-co]. is a method that is
related to LargeVis [@Tang2016-oz], and like t-SNE acts on the k-nearest
neighbor graph. Its main differences are that it uses a different cost function
(cross entropy) which is optimized using stochastic gradient descent and
defines a different kernel for similarities in the low dimensional space. Due
to it's computational speed it's possible to generate UMAP embeddings in more
than three dimensions. It appears to suffer from the same perceptual issues
as t-SNE, however it supposedly preservers global structure better
than t-SNE [@Coenen2019-to].


## Visual Design

We propose using tours as part of an analyst's workflow when using embedding
methods like t-SNE. We have made extensive use of ensemble
graphics, that is aligning related plots alongside each other to provide
context. As we will see in the case studies, this allows analysts to quickly
compare views from embedding methods and allows them to see how the embedding
method alters the global structure of their data. Using ensembles allows the
use of interaction techniques, that allow analysts to perform cluster
orientation tasks via linking multiple views. This approach allows our
interface, to achieve the three principles for interactive high-dimensional
data visualisation outlined by @Buja1996-fk: finding gestalt, posing queries,
and making comparisons.

### Finding Gestalt: focus and context

To investigate latent structure and the shape of a high dimensional dataset, a
tour can be run without the use of an external embedding. It is often useful to
first run principal components on the input as an initial dimension reduction
step, and then tour a subset of those components instead, i.e. by selecting
them from a scree plot. The default tour layout is a scatter plot with an axis
layout displaying the magnitude and direction of each basis vector. Since the
tour is dynamic, it is often useful to be able to pause and highlight a
particular view. In our interface, brushing will pause the tour path, allowing
users to identify 'interesting' projections. The domain of the axis scales from
running a tour is called the half range, and is computed by rescaling the input
data onto hyper-dimensional unit cube. We bind the half range to a mouse wheel
event, allowing a user to pan and zoom on the tour view dynamically. This is
useful for peeling back dense clumps of points to reveal structure.

### Posing Queries: multiple views, many contexts

We have combined the tour view in a side by side layout with a scatter plot
view that represents the output of an embedding algorithm.  These views are
linked; analysts can brush regions or highlight collections of points in either
view. Linked highlighting can be performed when points have been previously
labelled according to some discrete structure, i.e. cluster labels are
available. This is achieved via the analyst clicking on groups in the legend,
which causes unselected groupings to have their points become less opaque.
Consequently, simple linked highlighting can alleviate a known downfall of
methods such as UMAP or t-SNE: that is distances between clusters are
misleading. By highlighting corresponding clusters in the tour view, the
analyst can see the relationship between clusters, and therefore obtain a more
accurate representation of the topology of their data.

Simple linked brushing is achieved via mouse-click and drag movements. By
default, when brushing occurs in the tour view, the current projection is
paused and corresponding points in the embedding view are highlighted.
Likewise, when brushing occurs in the embedding view, corresponding points in
the tour view are highlighted. In this case, an analyst can use brushing for
manually identifying clusters and verifying cluster locations and shapes:
brushing in the embedding view gives analysts a sense of the shape and
proximity of cluster in high-dimensional space.

### Making comparisons: revising embeddings

As mentioned previously, when using any DR method, we are assuming the
embedding is representative of the high-dimensional dataset it was computed
from. Defining what it means for embedding to be 'representative` or 'faithful'
to high-dimensional data is ill-posed and depends on the underlying task an
analyst is trying to achieve. At the very minimum, we are interested in
distortions and diffusions of the high-dimensional data. Distortions occur when
points that are near each other in the embedding view are far from each other
in the original dataset. This implies that the embedding is not continuous.
Diffusions occur when points are far from each other in the embedding view are
near in the original data.  Whether, points are near or far is reliant on the
distance metric used; distortions and diffusions can be thought of as the
preservation of distances or the nearest neighbours graphs between the
high-dimensional space and the embedding space. As distances can be noisy in
high-dimensions, ranks can be used instead as has been proposed by ...
Identifying distortions and diffusions allows an analyst to investigate the
quality of their embedding and revise them iteratively. We propose this can be
done visually using our side-by-side tour and embedding views. Some quality
checks, such as finding distortions can be identified using simple linked
brushing, however we can also interrogate them via spatial brushes and brush
composition.

 Look up nearest neighbours graph from points that lie in a brushing region.

Highlight the corresponding neighbours using colour or transparency in the
linked view.


- The $k$ nearest neighbours graph can be pre-computed quickly, for either $X$, $Y$ or both.
- Instead of using the neighbour indices, we could use the neighbour distances instead.
- Composition of multiple brushes could be used to show where there are matches/mismatches between nearest neighbour graphs.

Using a linked neighbourhood brush, we can visually investigate the nearest
neighbour relationships in the high-dimensional space via brushing in the
embedding view. The user can select the number of nearest neighbours directly,
and modify the distance metric used for determining the neighbours. This allows
users to interrogate the stability of clusters generated in the embedding view.
Multiple brushes can be used to pose queries in either the tour view or the
embedding view; interface controls allow these brushes to combine using logical
operators such as 'and', 'or', or 'not'. The use of linked brushing goes beyond
simple colour highlights, allowing analysts to get a more holistic view of the
effect of an embedding algorithm.


## Software Infrastructure

We have implemented the above design as an open source R package called
`liminal` [@baseR]. The package allows analysts to construct concatenated
visualisations, via the Vega-Lite grammar of interactive graphics (using
`vegawidget` package) and provides an interface for constructing brushes and
manipulating tour paths using the `shiny` and `tourr` packages. `liminal` also
provides a stand-alone interface to the tour, in addition to the linked
scatterplot interfaces discussed above. Below we display the `liminal` API, and
our approach to generating tour paths and user interactions.


### Tours as a streaming data problem

- don't need to realise the entire sequence
- instead generate new bases according to a fixed frame rate
- allows user interactions to play/pause on the current view

### Linking views via brushes



## Case Studies

The next section steps through three use cases of our approach 
using simulated and real data. In the first case study, we step through
data that lies on tree structure (similar to trajectories in single
cell transcriptomics) to that highlights an iterative approach to modifying
t-SNE views based on what we learn in the high-dimensional space. We show
how through simple linked brushing, we can see when the assumptions
of continuity of the embedding map are broken. In the second case study,
we explore a well-clustered single cell transcriptomics data of mice retina...
Finally, in the last case study we explore a subset of the QuickDraw dataset,
we use `liminal` to identify instances of images from different labels that
appear similiar. 

### Case Study 1: Exploring tree structured data with tours and t-SNE



(ref:fake-trees) Example high-dimensional tree shaped data, $n = 3000$ and $p = 100$. The data lies 2-d tree like structure consisting of ten branches. This data is available in the `phateR` package and is simulated via diffusion-limited aggregation (a random walk along the branches of the tree) with Gaussian noise added [@Moon2019-ce].  

```{r fake-trees, echo = FALSE, out.width = "50%", fig.align = "center", fig.cap="(ref:fake-trees)"}
knitr::include_graphics(here::here("img", "tree.png"))
```


### Case Study 2: Clustering single cell RNA-seq data

### Case Study 3: QuickDraw images


## Discussion


## Acknowledgements


## References
